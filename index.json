[{"content":"","date":"1 January 0001","permalink":"/","section":"","summary":"","title":""},{"content":" About Me # I\u0026rsquo;m passionate about data science and eager to kick-start my career in this exciting field. Here\u0026rsquo;s a brief overview of my background and skills:\nEducation: I hold a degree in Civil Engineering, which has equipped me with strong problem-solving abilities and analytical thinking skills.\nTransition to Data Science: My curiosity for data and its potential led me to explore data science further. I saw the opportunity to leverage my analytical and quantitative skills gained from my engineering studies.\nSkills: I have developed proficiency in coding, statistics, and machine learning through self-study and practical projects. Here are the current key skills I possess:\nProgramming: Python, SQL, Bash Data Manipulation: Pandas, NumPy Data Visualization: Tableau, Matplotlib, Seaborn Machine Learning: Scikit-learn, Xgboost, LightGBM Tools: Git, Apache Airflow,Mage-ai, MongoDB Data Analysis: ETL, Hypothesis Testing, Clustering, PCA Continuous Learning: I have a strong commitment to continuous learning and stay up-to-date with the latest trends and techniques in data science. I am eager to expand my knowledge and skills through real-world projects and mentorship.\nPassion for Impact: I am driven by the potential of data science to drive meaningful change and solve complex problems. I am eager to apply my skills to make a positive impact on businesses and society.\nI am excited to connect with professionals in the field and explore opportunities to contribute and grow in the data science community. Please feel free to reach out to me via email or connect with me on LinkedIn.\n","date":"1 January 0001","permalink":"/about-me/","section":"","summary":"About Me # I\u0026rsquo;m passionate about data science and eager to kick-start my career in this exciting field. Here\u0026rsquo;s a brief overview of my background and skills:\nEducation: I hold a degree in Civil Engineering, which has equipped me with strong problem-solving abilities and analytical thinking skills.","title":"About Me"},{"content":"","date":"1 January 0001","permalink":"/tags/apriori-algorithm/","section":"Tags","summary":"","title":"Apriori Algorithm"},{"content":"","date":"1 January 0001","permalink":"/tags/association-rule/","section":"Tags","summary":"","title":"Association rule"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":" Description # This project focuses on predicting whether a client will subscribe to a term deposit or not. Using the Bank Marketing Dataset obtained from the UCI Machine Learning Repository, we employed classification models, including Logistic Regression, Random Forest, and LightGBM, to address class imbalance and generate accurate predictions. Insights from exploratory data analysis highlighted factors influencing subscription rates, and the evaluation metric of recall prioritized capturing potential subscribers. Hyperparameter tuning further improved model performance.\nKey Insights: # Students had the highest subscription rate, despite being a smaller group compared to others Single individuals exhibited a higher proportion of subscribers. Illiterate individuals subscribed more than other groups, while clients with basic education had the lowest subscription rate. Certain months, such as March, December, September, and October, had higher subscription rates, with Thursday being the day of the week with the highest subscription. Clients who subscribed had longer median contact durations compared to non-subscribers. Clients with no previous campaign history had a higher subscription rate. Models Used: # Logistic Regression Random Forest Classifier LGBM Classifier Evaluation Results: # After addressing class imbalance using class reweighing and random undersampling techniques, the models showed improved recall values for the positive class. After hyperparameter tuning using the Optuna library, the final model(LightGBM model) achieved a recall of approximately 73% on a separate test set, demonstrating its effectiveness in identifying potential subscribers.\nTableau Dashboard: # I created a comprehensive Tableau dashboard to visualize key insights. The dashboard provides a user-friendly interface for exploring the data and gaining a deeper understanding of the project\u0026rsquo;s findings. GitHub Repository # For a detailed exploration of the analysis please visit my GitHub repository. The repository contains the Jupyter Notebook,codes, and associated files related to this project.\nConclusion: # By predicting term deposit subscriptions, the developed model can help banks optimize their marketing strategies and enhance customer targeting. Feel free to explore the project details and Tableau dashboard to gain insights into customer behavior and the predictive power of the models.\n","date":"1 January 0001","permalink":"/projects/bank-term-prediction-project/","section":"Portfolio","summary":"Employing classification models and EDA, this project predicts term deposit subscriptions, achieves high recall, and provides actionable insights for optimizing marketing strategies.","title":"Banking on Success: Predicting Term Deposit Subscriptions"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/tags/class-imbalance-handling/","section":"Tags","summary":"","title":"Class Imbalance Handling"},{"content":"","date":"1 January 0001","permalink":"/tags/customer-segmentation/","section":"Tags","summary":"","title":"customer segmentation"},{"content":"","date":"1 January 0001","permalink":"/tags/data-cleaning/","section":"Tags","summary":"","title":"Data cleaning"},{"content":"","date":"1 January 0001","permalink":"/tags/data-visualization/","section":"Tags","summary":"","title":"Data Visualization"},{"content":" Description # This project focuses on customer segmentation using RFM (Recency, Frequency, Monetary) analysis and the K-means clustering algorithm. By analyzing customer transaction data from the Online Retail dataset, we aim to gain insights into customer behavior, identify valuable customer segments, and develop targeted marketing strategies.\nKey Features # RFM Analysis: Utilized the RFM framework to measure customer recency, frequency, and monetary value. K-means Clustering: Applied the K-means algorithm to partition customers into distinct segments based on their purchasing patterns. Optimal Cluster Selection: Utilized the elbow curve and silhouette score to determine the optimal number of clusters. Segmentation Recommendations: Provided actionable insights and recommendations for each customer segment. Result # Clusters RFM values The customer segmentation resulted in three distinct segments:\nCluster 0: Recent but Low-Value Customers\nApproximately 24% of total revenue. Lower frequency and monetary value. Recommendations: Implement targeted promotions, personalized offers, and loyalty programs to increase engagement and loyalty. Cluster 1: High-Value and Loyal Customers\nMost valuable segment. High frequency of purchases and significant monetary value. Recommendations: Provide personalized recommendations, exclusive deals, and premium services to enhance their experience and maintain their loyalty. Cluster 2: Churn-Prone Customers\nApproximately 5% of total revenue. Longer recency period, lower frequency, and monetary value. Recommendations: Adopt special strategies to prevent churn, such as win-back campaigns, re-engagement strategies, or special discounts. snake plot # The snake plot shows that cluster 1 has lowest recency ,while cluster 2 has the highest recency, with cluster 0 being in between. with respect to frequency and monetary values, cluster 1 has significantly high values compared to cluster 0 and cluster 2, with cluster 2 having the lowest values among the clusters. GitHub Repository # For a detailed exploration of the analysis please visit my GitHub repository. The repository contains the Jupyter Notebook,codes, and associated files related to this project.\nConclusion # By identifying distinct customer segments and understanding their preferences and behaviors, businesses can optimize their marketing efforts and enhance customer satisfaction. Feel free to explore the project and its findings to gain insights into customer segmentation and personalized marketing strategies.\n","date":"1 January 0001","permalink":"/projects/customer-segmentation/","section":"Portfolio","summary":"utilized RFM analysis and K-means clustering to segment customers based on their purchasing patterns, enabling targeted marketing strategies and personalized campaigns for business growth.","title":"Driving Business Growth: Unlocking Customer Insights with RFM Analysis and K-means Clustering"},{"content":"","date":"1 January 0001","permalink":"/tags/duckdb/","section":"Tags","summary":"","title":"DuckDb"},{"content":"","date":"1 January 0001","permalink":"/tags/ec2/","section":"Tags","summary":"","title":"EC2"},{"content":"","date":"1 January 0001","permalink":"/tags/etl/","section":"Tags","summary":"","title":"ETL"},{"content":"","date":"1 January 0001","permalink":"/tags/exploratory-data-analysis/","section":"Tags","summary":"","title":"Exploratory Data Analysis"},{"content":"","date":"1 January 0001","permalink":"/tags/feature-engineering/","section":"Tags","summary":"","title":"Feature Engineering"},{"content":"","date":"1 January 0001","permalink":"/tags/hyperparameter-tunning/","section":"Tags","summary":"","title":"Hyperparameter tunning"},{"content":"","date":"1 January 0001","permalink":"/tags/jupysql/","section":"Tags","summary":"","title":"JupySQL"},{"content":"","date":"1 January 0001","permalink":"/tags/jupyter-notebook/","section":"Tags","summary":"","title":"Jupyter Notebook"},{"content":"","date":"1 January 0001","permalink":"/tags/kmeans-clustering/","section":"Tags","summary":"","title":"Kmeans clustering"},{"content":"","date":"1 January 0001","permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine learning"},{"content":"","date":"1 January 0001","permalink":"/tags/mage-ai/","section":"Tags","summary":"","title":"Mage-ai"},{"content":"","date":"1 January 0001","permalink":"/tags/market-basket-analysis/","section":"Tags","summary":"","title":"Market Basket Analysis"},{"content":"","date":"1 January 0001","permalink":"/tags/mcda/","section":"Tags","summary":"","title":"MCDA"},{"content":"","date":"1 January 0001","permalink":"/tags/numpy/","section":"Tags","summary":"","title":"Numpy"},{"content":"","date":"1 January 0001","permalink":"/tags/pandas/","section":"Tags","summary":"","title":"Pandas"},{"content":" Welcome to my data science portfolio! # Here, you\u0026rsquo;ll find a collection of my projects, showcasing my expertise in data analysis, data visualization, machine learning, and more.\n","date":"1 January 0001","permalink":"/projects/","section":"Portfolio","summary":"Welcome to my data science portfolio! # Here, you\u0026rsquo;ll find a collection of my projects, showcasing my expertise in data analysis, data visualization, machine learning, and more.","title":"Portfolio"},{"content":"","date":"1 January 0001","permalink":"/tags/postgresql/","section":"Tags","summary":"","title":"Postgresql"},{"content":"","date":"1 January 0001","permalink":"/tags/quicksight/","section":"Tags","summary":"","title":"Quicksight"},{"content":"","date":"1 January 0001","permalink":"/tags/ranking-algorithm/","section":"Tags","summary":"","title":"Ranking Algorithm"},{"content":" Description # This project focuses on ranking the best college towns for remote workers. By leveraging AWS services(S3, EC2,RDS and Quicksight), utilising ETL (Extract, Transform, Load) using Mage-AI data pipeline, we integrate and analyze data from various sources to provide valuable insights. Our goal is to empower remote workers in selecting the most suitable college towns based on criteria such as internet speed, median income, coworking spaces, cost of living, and active mobility.\nArchitecture # Data stack/Tools used # Python BeautifulSoup \u0026amp; Selenium framework for webscraping Mage-ai for data pipeline Aws services: EC2 for VM, S3 buckets for storing raw data, Postgresql on RDS for storing transformed data, Quicksight for visualization. Dbeaver for database management/SQL Editor Data sources # \u003c!DOCTYPE html\u003e Data Sources Data Source Data obtained (for college towns) Wikipedia US College Towns Names Speedtest Internet Speed Bestplaces Demographic Data (Population, Median Age, Median Income, etc.) Walkscore Walkscore and Bikescore CityFeet Number of Coworking Spaces Data Integration and Workflow: # Data was collected from various sources, such as Wikipedia, Speedtest, Bestplaces, Walkscore, and CityFeet, using Python with BeautifulSoup and Selenium frameworks for web scraping. The collected data is stored in AWS S3 buckets. Mage-AI data pipeline was used for ETL, the data was cleaned and prepared during transformation phase before loading,ensuring data integrity and consistency. The transformed data is then loaded into a PostgreSQL database on AWS RDS, enabling efficient storage and retrieval.\nKey Insights From Analysis # After loading the data to postgresql database, the following insights were obtained from the analysis done in the database:\nThe college town with the highest number of coworking spaces is Austin, Texas, with 31 coworking spaces. There are 73 college towns with a median income above the national average of $70,785. Some college towns with high median incomes include Claremont, California ($89,648), Wellesley, Massachusetts ($159,615), Princeton, New Jersey ($116,875), and Aurora, New York ($72,787). The average cost of living index for college towns with populations between 50,000 and 100,000 is 101.66. Montreat, North Carolina has the highest number of eateries per capita among college towns, with a ratio of 0.87 eateries per person. Other towns with high ratios include Misenheimer, North Carolina (0.42), and Due West, South Carolina (0.38). The average download speed in college towns is approximately 202.84 Mbps, while the average upload speed is around 29.73 Mbps. There is a moderate positive correlation (correlation coefficient of 0.60) between the median income and the cost of living index in college towns. This suggests that as the median income increases, the cost of living tends to increase as well. Here\u0026rsquo;s a table for the top 5 most affordable college towns based on the cost of living index: College Towns Cost of Living Index Rank Itta Bena, Mississippi 65.3 1 Marion, Indiana 66.1 2 Youngstown, Ohio 66.1 2 Pittsburg, Kansas 66.5 4 Portales, New Mexico 68.0 5 Ranking Method # Ranking Methodology: To rank the best college towns for remote workers, we employ a weighted sum model. We assign appropriate weights to each criterion , based on the perceived importance of these criteria to remote workers.Factors such as the number of coworking spaces per capita, unemployment rate, active mobility score, and more are carefully evaluated and assigned weights accordingly.This approach ensures that the model reflects the relative significance of each criterion, allowing for a more accurate and informed decision-making process when selecting a college town for remote work.\nTableau Dashboard # I created an interactive Tableau dashboard to visualize key insights. The dashboard provides a user-friendly interface for exploring the data and gaining a deeper understanding of the project\u0026rsquo;s findings. open image in another tab to enlarge it GitHub Repository # For a detailed exploration of the analysis please visit my GitHub repository. The repository contains the Web scraping scripts, Mage-ai scripts,codes, and associated files related to this project.\nConclusion # The college town data integration and analysis project provides valuable insights into the factors that contribute to the appeal and livability of college towns for remote workers. By leveraging AWS services, Mage-AI data pipeline, and ETL techniques, we enable individuals and businesses to make informed decisions when selecting a college town. The project\u0026rsquo;s findings empower users with the necessary information to foster better choices and enhance overall satisfaction\n","date":"1 January 0001","permalink":"/projects/college-towns-etl-project/","section":"Portfolio","summary":"A comprehensive end-to-end data integration and analysis project focused on ranking top college towns for remote workers. It utilizes AWS services, including S3, EC2,RDS and Quicksight, along with the Mage data pipeline tool for ETL operations and Tableau.","title":"Ranking the Best College Towns for Remote Workers: Data Integration, ETL, Analysis, and Insights with AWS and Mage-AI"},{"content":"","date":"1 January 0001","permalink":"/tags/rds/","section":"Tags","summary":"","title":"RDS"},{"content":"","date":"1 January 0001","permalink":"/tags/rfm-analysis/","section":"Tags","summary":"","title":"RFM analysis"},{"content":"","date":"1 January 0001","permalink":"/tags/s3/","section":"Tags","summary":"","title":"S3"},{"content":"","date":"1 January 0001","permalink":"/tags/scikit-learn/","section":"Tags","summary":"","title":"Scikit-learn"},{"content":"","date":"1 January 0001","permalink":"/tags/selenium-framework/","section":"Tags","summary":"","title":"Selenium framework"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/tags/skcriteria/","section":"Tags","summary":"","title":"Skcriteria"},{"content":"","date":"1 January 0001","permalink":"/tags/snake-plot/","section":"Tags","summary":"","title":"snake plot"},{"content":"","date":"1 January 0001","permalink":"/tags/sql/","section":"Tags","summary":"","title":"SQL"},{"content":"","date":"1 January 0001","permalink":"/tags/tableau/","section":"Tags","summary":"","title":"Tableau"},{"content":"","date":"1 January 0001","permalink":"/tags/tableau-dashboard/","section":"Tags","summary":"","title":"Tableau dashboard"},{"content":"","date":"1 January 0001","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" Introduction # Market Basket Analysis is a powerful data mining technique that uncovers associations and patterns within customer transactions, enabling retailers to optimize their business strategies. By analyzing customer purchase patterns, this analysis helps identify which items are frequently bought together or sequentially, providing insights for product placement, cross-selling, and targeted marketing. In this project, we applied Market Basket Analysis to uncover hidden patterns and associations within a dataset of customer transactions, allowing us to generate meaningful and actionable insights.\nDataset # The Online Retail Data Set contains a wealth of information about customer transactions, including invoice details, purchased items, quantities, and customer IDs. By exploring this dataset and analyzing purchasing patterns, we can identify relationships between different products and gain valuable insights. We performed necessary data cleaning and transformations to prepare the data for Market Basket Analysis using the Apriori algorithm.\nInsights from Exploratory Data Analysis # The \u0026ldquo;Regency Cakestand 3 Tier\u0026rdquo; emerged as the top-selling item based on purchase frequency. REGENCY CAKESTAND 3 TIER is the most bought item in the dataset The day with the highest number of items purchased was 2011-11-14, and November 2011 recorded the highest overall purchase volume. November, 2011 was the month with highest items sold in the entire dataset The majority of purchases were made by customers in the United Kingdom, followed by Germany and other countries. Saudi Arabia, Bahrain, Czech Republic, Brazil, and Lithuania had the lowest purchase rates, indicating fewer customers in these regions. The most expensive item in the dataset was the \u0026ldquo;VINTAGE RED KITCHEN CABINET\u0026rdquo; with a unit price of 295. Customer ID 14646 had the highest spending among all customers. Open this image in another tab to enlarge it. Apriori Algorithm and Association Rules # The Apriori algorithm, a popular association rule mining technique, efficiently discovers frequent itemsets to generate meaningful association rules. Association rules capture relationships between items based on their co-occurrence in transactions and include measures like support, confidence, and lift.\nIn this analysis, we generated a total of 646 association rules. To focus on meaningful associations, we applied a minimum lift threshold of 1.01 during the pruning stage of the Apriori algorithm. Lift values greater than 1 indicate significant positive associations, aligning with our objective of uncovering meaningful relationships among items.\nThe first five rows of the association rule table is given below:\nantecedents consequents antecedent support consequent support support confidence lift leverage conviction (PACK OF 72 RETROSPOT CAKE CASES) (60 CAKE CASES DOLLY GIRL DESIGN) 0.056390 0.019399 0.010357 0.183673 9.468004 0.009263 1.201236 (60 CAKE CASES DOLLY GIRL DESIGN) (PACK OF 72 RETROSPOT CAKE CASES) 0.019399 0.056390 0.010357 0.533898 9.468004 0.009263 2.024473 (72 SWEETHEART FAIRY CAKE CASES) (60 TEATIME FAIRY CAKE CASES) 0.027455 0.036004 0.012111 0.441118 12.251928 0.011122 1.724864 (60 TEATIME FAIRY CAKE CASES) (72 SWEETHEART FAIRY CAKE CASES) 0.036004 0.027455 0.012111 0.336377 12.251928 0.011122 1.465509 (60 TEATIME FAIRY CAKE CASES) (PACK OF 60 DINOSAUR CAKE CASES) 0.036004 0.028989 0.012221 0.339422 11.708442 0.011177 1.469940 Results # The generated association rules provide valuable insights for various business applications. These rules can be utilized to identify patterns, optimize store layouts, and select items for cross-selling purposes. For instance, we can gain valuable insights into products frequently purchased together, enhancing marketing strategies, optimizing store layouts, and implementing cross-selling initiatives.\nExample of a discovered association rule:\n\u0026ldquo;If a customer purchases POPPY\u0026rsquo;S PLAYHOUSE LIVINGROOM , they are highly likely to also purchase POPPY\u0026rsquo;S PLAYHOUSE KITCHEN.\u0026rdquo;\nWe also created a simple market basket analysis-based recommendation system using item stock codes, enabling personalized recommendations for customers. Open this image in another tab to enlarge it. GitHub Repository # For a detailed exploration of the analysis please visit my GitHub repository. The repository contains the Jupyter Notebook,codes, and associated files related to this project.\n","date":"1 January 0001","permalink":"/projects/market-basket-analysis-project/","section":"Portfolio","summary":"Uncovered patterns in customer transactions using market basket analysis for actionable retail insights, cross-selling strategies, and personalized recommendations.","title":"Uncovering Shopping Secrets: Market Basket Analysis for Retail Optimization"},{"content":" Project Description # In this data analysis project, I performed exploratory data analysis using SQL in Jupyter Notebook to gain insights into traffic collisions in California. Leveraging JupyterSQL and DuckDB, I conducted a comprehensive examination of the California Traffic Collision Data obtained from SWITRS. This dataset covers collisions from January 1st, 2001 until mid-2021, comprising approximately 9,424,334 collision records.\nProject Highlights # Exploration and Analysis # (hover over the charts to interact with it)\nExplored the time duration covered by the database, spanning from January 1, 2001, to June 3, 2021.\nAnalyzed the total number of collisions recorded, revealing a staggering count of approximately 9.4 million collisions.\nIdentified the year with the highest collisions recorded, with 2002 ranking at the top, followed by 2003 and 2004. Investigated the counties with the highest collisions, with Los Angeles leading with over 2 million entries. Examined the impact of alcohol involvement, finding that approximately 10.02% of collisions involved alcohol.\nExplored the gender distribution of parties involved, highlighting that males accounted for 61.27%, females for 38.7%, and non-binary individuals for 0.02% of the total. Uncovered the most frequent PCF (Primary Collision Factor) violation category, identifying speeding as the primary factor. Analyzed the relationship between weather conditions, road surfaces, and lighting with traffic collisions. Explored hit-and-run cases, identifying the prevalence of non-hit and run incidents, misdemeanors, and felonies. Investigated the involvement of different vehicle makes in collisions, ranking the top 7 most frequent makes. Analyzed the age distribution of victims, revealing higher frequencies among individuals aged 10-20 and 20-30. GitHub Repository # For a detailed exploration of the analysis and access to the SQL code, please visit my GitHub repository. The repository contains the Jupyter Notebook, SQL queries, and associated files related to this project.\nConclusion # This SQL analysis project provides valuable insights into traffic collisions in California, covering various aspects such as time duration, total collisions, counties, alcohol involvement, gender distribution, PCF violation categories, weather conditions, and much more.\n","date":"1 January 0001","permalink":"/projects/sql-car-collision-project/","section":"Portfolio","summary":"Exploratory data analysis of California Traffic Collisions data using SQL. Obtained insights into collision patterns and trends.","title":"Unveiling Insights: Exploratory Data Analysis of California Traffic Collisions using SQl"},{"content":" Description # The Used Car Price Prediction project uses machine learning to accurately estimate the prices of used cars. By analyzing car features like brand, model, year, and performance indicators, the project empowers buyers and sellers with reliable price estimates. The dataset, obtained by scraping a popular car marketplace and consisting of around 19,000 cars, forms the foundation for exploring the relationships between different car attributes and their impact on pricing dynamics\nWork flow # graph LR A[Data Collection through web scraping] --\u003e B[Data Cleaning] B --\u003e C[Exploratory Data Analysis and Modeling] C --\u003e D[Flask Web App] Key Insights # Toyota is the most frequent used car brand in the dataset. this is true since the most popular car brand in South Africa is Toyota. After Toyota is Volkswagen and the rest. SUV dominates as the most popular used car brand sold on Autotrader South Africa. With regards to color, the top 3 colors of used cars on the website are white,silver and grey. Open this image in another tab to enlarge it There are more used cars with automatic transmission compared to the manual. with respect to fuel type, cars with petrol fuel type are most frequent, a very small number of cars have hybrid or electric fuel type Open this image in another tab to enlarge it The median prices of used cars manufactured in 2018 were the lowest compared to other years Cars with all-wheel drive have higher median price compared to other driven wheels type. Cars with v12 cylinder layout have significant higher median price compared to the rest. This is expected as cars with V12 cylinders are typically high-end, luxury vehicles that are known for their exceptional performance and high price tags Models Used # SVR KNeighborsRegressor RandomForestRegressor XGBRegressor LGBMRegressor Results # The best-performing model, XGBRegressor, achieves a mean absolute error (MAE) of 29,908.83 and an R-squared score of 0.94 on a separate test set.\nMetric Value MAE on test set 29908.83 R2 score 0.94 Flask Web App # To facilitate seamless price predictions, a user-friendly Flask web application is developed. Users can input relevant car details, such as brand, model, year, and performance indicators, and obtain instant price estimates based on the trained machine learning model. Open this image in another tab to enlarge it Tableau Dashboard # An interactive Tableau dashboard was made. (visit the link to interact with the dashboard)\nGitHub Repository # For a detailed exploration of the analysis please visit my GitHub repository. The repository contains the Jupyter Notebook,codes, and associated files related to this project.\nConclusion # The Used Car Price Prediction project demonstrates the power of machine learning in accurately predicting the prices of used cars. By leveraging data analysis and regression models, this project empowers buyers and sellers with reliable price estimates, facilitating informed decision-making in the automotive market.\n","date":"1 January 0001","permalink":"/projects/used-car-price-prediction-project/","section":"Portfolio","summary":"This project utilizes a dataset of over 19,000 scraped used car listings to predict prices. The XGBoost model achieves an MAE of 29,908.83 and an impressive R-squared score of 0.94. Gain insights from exploratory data analysis and experience seamless price prediction through the user-friendly Flask web app.","title":"Used Car Price Prediction"},{"content":"","date":"1 January 0001","permalink":"/tags/web-scraping/","section":"Tags","summary":"","title":"Web scraping"}]